{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e99dfb33-64f0-416b-8688-806fb0f3e26b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### extract_aind_output_shijia_ks4_addwaveform.ipynb\n",
    "#### Extract output for KS4 pipeline: \n",
    "#### Note: only works with spikeInterface 0.102.2+\n",
    "\n",
    "##### Everything we have for KS2.5 pipeline\n",
    "1. Merge multiple segments from NP2\n",
    "2. Extract curated units (who â€œmade the cutâ€)\n",
    "3. Extract quality metrics and waveforms from postprocessing\n",
    "4. Extract spike times from the original â€œspikesâ€ folder, filtered by curated unit IDs\n",
    "5. So your final outputs represent curated units with the waveforms/QMs from postprocessing.\n",
    "6. Actually, input_folder and raw_rec are not needed\n",
    "\n",
    "##### Additional outputs for visualization\n",
    "##### Shijia L., 2025/10/05\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a0df46a-aaf0-4d0e-a6ee-8ccb2bdfd155",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpikeInterface version: 0.102.1\n"
     ]
    }
   ],
   "source": [
    "import spikeinterface\n",
    "\n",
    "print(\"SpikeInterface version:\", spikeinterface.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8e58e0-aac8-40be-a26c-b00900bf7659",
   "metadata": {},
   "source": [
    "**Environment:** Python 3.9  \n",
    "**SpikeInterface:** 0.102.1\n",
    "**Created on:** 2025-03-28  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a28a7954-2634-4fb6-afe5-94e617962d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Session: 20250908_M373_4W50_g0 ===\n",
      "â†’ using output folder: 20250908_M373_4W50_g0_output\n",
      " â€¢ [1/1] /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20250908_M373_4W50_g0_output/preprocessed/block0_imec0.ap_recording1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home13/shliu/.conda/envs/spikeInterface/lib/python3.12/site-packages/spikeinterface/core/base.py:1109: UserWarning: Versions are not the same. This might lead to compatibility errors. Using spikeinterface==0.101.2 is recommended\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â†’ 565 units\n",
      " âœ… Total units: 565\n",
      "ğŸ¯ Wrote: spike_times.npy, spike_clusters.npy, cluster_group.tsv, cluster_info.tsv (+ template_metrics.tsv, best_channel_templates.npy)\n",
      "ğŸ§¾ Saved analysis_meta.json in /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20250908_M373_4W50_g0_output/AIND_20250908_M373_4W50_g0\n",
      "\n",
      "[diagnostics] AIND folder: /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20250908_M373_4W50_g0_output/AIND_20250908_M373_4W50_g0\n",
      "âœ… cluster_info.tsv rows=565, cols=45\n",
      "âœ… template_metrics.tsv rows=565, cols=12\n",
      "ğŸ‰ Template metrics present in cluster_info (examples): ['exp_decay', 'half_width', 'num_negative_peaks', 'num_positive_peaks', 'peak_to_valley', 'peak_trough_ratio', 'recovery_slope', 'repolarization_slope']\n",
      "ğŸ”— Join keys â€” template_metrics unique=565, cluster_info unique=565\n",
      "\n",
      "=== Session: 20250909_M373_4W50_g0 ===\n",
      "â†’ using output folder: 20250909_M373_4W50_g0_output\n",
      " â€¢ [1/1] /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20250909_M373_4W50_g0_output/preprocessed/block0_imec0.ap_recording1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home13/shliu/.conda/envs/spikeInterface/lib/python3.12/site-packages/spikeinterface/core/base.py:1109: UserWarning: Versions are not the same. This might lead to compatibility errors. Using spikeinterface==0.101.2 is recommended\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â†’ 633 units\n",
      " âœ… Total units: 633\n",
      "ğŸ¯ Wrote: spike_times.npy, spike_clusters.npy, cluster_group.tsv, cluster_info.tsv (+ template_metrics.tsv, best_channel_templates.npy)\n",
      "ğŸ§¾ Saved analysis_meta.json in /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20250909_M373_4W50_g0_output/AIND_20250909_M373_4W50_g0\n",
      "\n",
      "[diagnostics] AIND folder: /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20250909_M373_4W50_g0_output/AIND_20250909_M373_4W50_g0\n",
      "âœ… cluster_info.tsv rows=633, cols=45\n",
      "âœ… template_metrics.tsv rows=633, cols=12\n",
      "ğŸ‰ Template metrics present in cluster_info (examples): ['exp_decay', 'half_width', 'num_negative_peaks', 'num_positive_peaks', 'peak_to_valley', 'peak_trough_ratio', 'recovery_slope', 'repolarization_slope']\n",
      "ğŸ”— Join keys â€” template_metrics unique=633, cluster_info unique=633\n",
      "\n",
      "=== Session: 20250910_M373_4W50_g0 ===\n",
      "â†’ using output folder: 20250910_M373_4W50_g0_output\n",
      " â€¢ [1/1] /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20250910_M373_4W50_g0_output/preprocessed/block0_imec0.ap_recording1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home13/shliu/.conda/envs/spikeInterface/lib/python3.12/site-packages/spikeinterface/core/base.py:1109: UserWarning: Versions are not the same. This might lead to compatibility errors. Using spikeinterface==0.101.2 is recommended\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â†’ 562 units\n",
      " âœ… Total units: 562\n",
      "ğŸ¯ Wrote: spike_times.npy, spike_clusters.npy, cluster_group.tsv, cluster_info.tsv (+ template_metrics.tsv, best_channel_templates.npy)\n",
      "ğŸ§¾ Saved analysis_meta.json in /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20250910_M373_4W50_g0_output/AIND_20250910_M373_4W50_g0\n",
      "\n",
      "[diagnostics] AIND folder: /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20250910_M373_4W50_g0_output/AIND_20250910_M373_4W50_g0\n",
      "âœ… cluster_info.tsv rows=562, cols=45\n",
      "âœ… template_metrics.tsv rows=562, cols=12\n",
      "ğŸ‰ Template metrics present in cluster_info (examples): ['exp_decay', 'half_width', 'num_negative_peaks', 'num_positive_peaks', 'peak_to_valley', 'peak_trough_ratio', 'recovery_slope', 'repolarization_slope']\n",
      "ğŸ”— Join keys â€” template_metrics unique=562, cluster_info unique=562\n",
      "\n",
      "=== Session: 20250911_M373_4W50_g0 ===\n",
      "â†’ using output folder: 20250911_M373_4W50_g0_output\n",
      " â€¢ [1/1] /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20250911_M373_4W50_g0_output/preprocessed/block0_imec0.ap_recording1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home13/shliu/.conda/envs/spikeInterface/lib/python3.12/site-packages/spikeinterface/core/base.py:1109: UserWarning: Versions are not the same. This might lead to compatibility errors. Using spikeinterface==0.101.2 is recommended\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â†’ 564 units\n",
      " âœ… Total units: 564\n",
      "ğŸ¯ Wrote: spike_times.npy, spike_clusters.npy, cluster_group.tsv, cluster_info.tsv (+ template_metrics.tsv, best_channel_templates.npy)\n",
      "ğŸ§¾ Saved analysis_meta.json in /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20250911_M373_4W50_g0_output/AIND_20250911_M373_4W50_g0\n",
      "\n",
      "[diagnostics] AIND folder: /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20250911_M373_4W50_g0_output/AIND_20250911_M373_4W50_g0\n",
      "âœ… cluster_info.tsv rows=564, cols=45\n",
      "âœ… template_metrics.tsv rows=564, cols=12\n",
      "ğŸ‰ Template metrics present in cluster_info (examples): ['exp_decay', 'half_width', 'num_negative_peaks', 'num_positive_peaks', 'peak_to_valley', 'peak_trough_ratio', 'recovery_slope', 'repolarization_slope']\n",
      "ğŸ”— Join keys â€” template_metrics unique=564, cluster_info unique=564\n",
      "\n",
      "=== Session: 20250912_M373_4W50_g0 ===\n",
      "â†’ using output folder: 20250912_M373_4W50_g0_output\n",
      " â€¢ [1/1] /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20250912_M373_4W50_g0_output/preprocessed/block0_imec0.ap_recording1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home13/shliu/.conda/envs/spikeInterface/lib/python3.12/site-packages/spikeinterface/core/base.py:1109: UserWarning: Versions are not the same. This might lead to compatibility errors. Using spikeinterface==0.101.2 is recommended\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â†’ 514 units\n",
      " âœ… Total units: 514\n",
      "ğŸ¯ Wrote: spike_times.npy, spike_clusters.npy, cluster_group.tsv, cluster_info.tsv (+ template_metrics.tsv, best_channel_templates.npy)\n",
      "ğŸ§¾ Saved analysis_meta.json in /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20250912_M373_4W50_g0_output/AIND_20250912_M373_4W50_g0\n",
      "\n",
      "[diagnostics] AIND folder: /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20250912_M373_4W50_g0_output/AIND_20250912_M373_4W50_g0\n",
      "âœ… cluster_info.tsv rows=514, cols=45\n",
      "âœ… template_metrics.tsv rows=514, cols=12\n",
      "ğŸ‰ Template metrics present in cluster_info (examples): ['exp_decay', 'half_width', 'num_negative_peaks', 'num_positive_peaks', 'peak_to_valley', 'peak_trough_ratio', 'recovery_slope', 'repolarization_slope']\n",
      "ğŸ”— Join keys â€” template_metrics unique=514, cluster_info unique=514\n",
      "\n",
      "=== Session: 20250915_M373_4W50_g0 ===\n",
      "â†’ using output folder: 20250915_M373_4W50_g0_output\n",
      " â€¢ [1/1] /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20250915_M373_4W50_g0_output/preprocessed/block0_imec0.ap_recording1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home13/shliu/.conda/envs/spikeInterface/lib/python3.12/site-packages/spikeinterface/core/base.py:1109: UserWarning: Versions are not the same. This might lead to compatibility errors. Using spikeinterface==0.101.2 is recommended\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â†’ 564 units\n",
      " âœ… Total units: 564\n",
      "ğŸ¯ Wrote: spike_times.npy, spike_clusters.npy, cluster_group.tsv, cluster_info.tsv (+ template_metrics.tsv, best_channel_templates.npy)\n",
      "ğŸ§¾ Saved analysis_meta.json in /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20250915_M373_4W50_g0_output/AIND_20250915_M373_4W50_g0\n",
      "\n",
      "[diagnostics] AIND folder: /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20250915_M373_4W50_g0_output/AIND_20250915_M373_4W50_g0\n",
      "âœ… cluster_info.tsv rows=564, cols=45\n",
      "âœ… template_metrics.tsv rows=564, cols=12\n",
      "ğŸ‰ Template metrics present in cluster_info (examples): ['exp_decay', 'half_width', 'num_negative_peaks', 'num_positive_peaks', 'peak_to_valley', 'peak_trough_ratio', 'recovery_slope', 'repolarization_slope']\n",
      "ğŸ”— Join keys â€” template_metrics unique=564, cluster_info unique=564\n",
      "\n",
      "=== Session: 20250916_M373_4W50LickNumBlock_g0 ===\n",
      "â†’ using output folder: 20250916_M373_4W50LickNumBlock_g0_output\n",
      " â€¢ [1/1] /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20250916_M373_4W50LickNumBlock_g0_output/preprocessed/block0_imec0.ap_recording1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home13/shliu/.conda/envs/spikeInterface/lib/python3.12/site-packages/spikeinterface/core/base.py:1109: UserWarning: Versions are not the same. This might lead to compatibility errors. Using spikeinterface==0.101.2 is recommended\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â†’ 558 units\n",
      " âœ… Total units: 558\n",
      "ğŸ¯ Wrote: spike_times.npy, spike_clusters.npy, cluster_group.tsv, cluster_info.tsv (+ template_metrics.tsv, best_channel_templates.npy)\n",
      "ğŸ§¾ Saved analysis_meta.json in /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20250916_M373_4W50LickNumBlock_g0_output/AIND_20250916_M373_4W50LickNumBlock_g0\n",
      "\n",
      "[diagnostics] AIND folder: /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20250916_M373_4W50LickNumBlock_g0_output/AIND_20250916_M373_4W50LickNumBlock_g0\n",
      "âœ… cluster_info.tsv rows=558, cols=45\n",
      "âœ… template_metrics.tsv rows=558, cols=12\n",
      "ğŸ‰ Template metrics present in cluster_info (examples): ['exp_decay', 'half_width', 'num_negative_peaks', 'num_positive_peaks', 'peak_to_valley', 'peak_trough_ratio', 'recovery_slope', 'repolarization_slope']\n",
      "ğŸ”— Join keys â€” template_metrics unique=558, cluster_info unique=558\n",
      "\n",
      "=== Session: 20250917_M373_4W50LickNumBlock_g0 ===\n",
      "â†’ using output folder: 20250917_M373_4W50LickNumBlock_g0_output\n",
      " â€¢ [1/1] /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20250917_M373_4W50LickNumBlock_g0_output/preprocessed/block0_imec0.ap_recording1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home13/shliu/.conda/envs/spikeInterface/lib/python3.12/site-packages/spikeinterface/core/base.py:1109: UserWarning: Versions are not the same. This might lead to compatibility errors. Using spikeinterface==0.101.2 is recommended\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â†’ 605 units\n",
      " âœ… Total units: 605\n",
      "ğŸ¯ Wrote: spike_times.npy, spike_clusters.npy, cluster_group.tsv, cluster_info.tsv (+ template_metrics.tsv, best_channel_templates.npy)\n",
      "ğŸ§¾ Saved analysis_meta.json in /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20250917_M373_4W50LickNumBlock_g0_output/AIND_20250917_M373_4W50LickNumBlock_g0\n",
      "\n",
      "[diagnostics] AIND folder: /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20250917_M373_4W50LickNumBlock_g0_output/AIND_20250917_M373_4W50LickNumBlock_g0\n",
      "âœ… cluster_info.tsv rows=605, cols=45\n",
      "âœ… template_metrics.tsv rows=605, cols=12\n",
      "ğŸ‰ Template metrics present in cluster_info (examples): ['exp_decay', 'half_width', 'num_negative_peaks', 'num_positive_peaks', 'peak_to_valley', 'peak_trough_ratio', 'recovery_slope', 'repolarization_slope']\n",
      "ğŸ”— Join keys â€” template_metrics unique=605, cluster_info unique=605\n",
      "\n",
      "=== Session: 20250918_M373_4W50LickNumBlock_g0 ===\n",
      "â†’ using output folder: 20250918_M373_4W50LickNumBlock_g0_output\n",
      " â€¢ [1/1] /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20250918_M373_4W50LickNumBlock_g0_output/preprocessed/block0_imec0.ap_recording1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home13/shliu/.conda/envs/spikeInterface/lib/python3.12/site-packages/spikeinterface/core/base.py:1109: UserWarning: Versions are not the same. This might lead to compatibility errors. Using spikeinterface==0.101.2 is recommended\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â†’ 440 units\n",
      " âœ… Total units: 440\n",
      "ğŸ¯ Wrote: spike_times.npy, spike_clusters.npy, cluster_group.tsv, cluster_info.tsv (+ template_metrics.tsv, best_channel_templates.npy)\n",
      "ğŸ§¾ Saved analysis_meta.json in /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20250918_M373_4W50LickNumBlock_g0_output/AIND_20250918_M373_4W50LickNumBlock_g0\n",
      "\n",
      "[diagnostics] AIND folder: /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20250918_M373_4W50LickNumBlock_g0_output/AIND_20250918_M373_4W50LickNumBlock_g0\n",
      "âœ… cluster_info.tsv rows=440, cols=45\n",
      "âœ… template_metrics.tsv rows=440, cols=12\n",
      "ğŸ‰ Template metrics present in cluster_info (examples): ['exp_decay', 'half_width', 'num_negative_peaks', 'num_positive_peaks', 'peak_to_valley', 'peak_trough_ratio', 'recovery_slope', 'repolarization_slope']\n",
      "ğŸ”— Join keys â€” template_metrics unique=440, cluster_info unique=440\n",
      "\n",
      "=== Session: 20250919_M373_4W50LickNumBlock_g0 ===\n",
      "â†’ using output folder: 20250919_M373_4W50LickNumBlock_g0_output\n",
      " â€¢ [1/1] /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20250919_M373_4W50LickNumBlock_g0_output/preprocessed/block0_imec0.ap_recording1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home13/shliu/.conda/envs/spikeInterface/lib/python3.12/site-packages/spikeinterface/core/base.py:1109: UserWarning: Versions are not the same. This might lead to compatibility errors. Using spikeinterface==0.101.2 is recommended\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â†’ 508 units\n",
      " âœ… Total units: 508\n",
      "ğŸ¯ Wrote: spike_times.npy, spike_clusters.npy, cluster_group.tsv, cluster_info.tsv (+ template_metrics.tsv, best_channel_templates.npy)\n",
      "ğŸ§¾ Saved analysis_meta.json in /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20250919_M373_4W50LickNumBlock_g0_output/AIND_20250919_M373_4W50LickNumBlock_g0\n",
      "\n",
      "[diagnostics] AIND folder: /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20250919_M373_4W50LickNumBlock_g0_output/AIND_20250919_M373_4W50LickNumBlock_g0\n",
      "âœ… cluster_info.tsv rows=508, cols=45\n",
      "âœ… template_metrics.tsv rows=508, cols=12\n",
      "ğŸ‰ Template metrics present in cluster_info (examples): ['exp_decay', 'half_width', 'num_negative_peaks', 'num_positive_peaks', 'peak_to_valley', 'peak_trough_ratio', 'recovery_slope', 'repolarization_slope']\n",
      "ğŸ”— Join keys â€” template_metrics unique=508, cluster_info unique=508\n",
      "\n",
      "=== Session: 20250922_M373_4W50LickNumBlock_g0 ===\n",
      "â†’ using output folder: 20250922_M373_4W50LickNumBlock_g0_output\n",
      " â€¢ [1/1] /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20250922_M373_4W50LickNumBlock_g0_output/preprocessed/block0_imec0.ap_recording1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home13/shliu/.conda/envs/spikeInterface/lib/python3.12/site-packages/spikeinterface/core/base.py:1109: UserWarning: Versions are not the same. This might lead to compatibility errors. Using spikeinterface==0.101.2 is recommended\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â†’ 571 units\n",
      " âœ… Total units: 571\n",
      "ğŸ¯ Wrote: spike_times.npy, spike_clusters.npy, cluster_group.tsv, cluster_info.tsv (+ template_metrics.tsv, best_channel_templates.npy)\n",
      "ğŸ§¾ Saved analysis_meta.json in /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20250922_M373_4W50LickNumBlock_g0_output/AIND_20250922_M373_4W50LickNumBlock_g0\n",
      "\n",
      "[diagnostics] AIND folder: /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20250922_M373_4W50LickNumBlock_g0_output/AIND_20250922_M373_4W50LickNumBlock_g0\n",
      "âœ… cluster_info.tsv rows=571, cols=45\n",
      "âœ… template_metrics.tsv rows=571, cols=12\n",
      "ğŸ‰ Template metrics present in cluster_info (examples): ['exp_decay', 'half_width', 'num_negative_peaks', 'num_positive_peaks', 'peak_to_valley', 'peak_trough_ratio', 'recovery_slope', 'repolarization_slope']\n",
      "ğŸ”— Join keys â€” template_metrics unique=571, cluster_info unique=571\n",
      "\n",
      "=== Session: 20250924_M373_FourStimuli_g0 ===\n",
      "â†’ using output folder: 20250924_M373_FourStimuli_g0_output\n",
      " â€¢ [1/1] /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20250924_M373_FourStimuli_g0_output/preprocessed/block0_imec0.ap_recording1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home13/shliu/.conda/envs/spikeInterface/lib/python3.12/site-packages/spikeinterface/core/base.py:1109: UserWarning: Versions are not the same. This might lead to compatibility errors. Using spikeinterface==0.101.2 is recommended\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â†’ 567 units\n",
      " âœ… Total units: 567\n",
      "ğŸ¯ Wrote: spike_times.npy, spike_clusters.npy, cluster_group.tsv, cluster_info.tsv (+ template_metrics.tsv, best_channel_templates.npy)\n",
      "ğŸ§¾ Saved analysis_meta.json in /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20250924_M373_FourStimuli_g0_output/AIND_20250924_M373_FourStimuli_g0\n",
      "\n",
      "[diagnostics] AIND folder: /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20250924_M373_FourStimuli_g0_output/AIND_20250924_M373_FourStimuli_g0\n",
      "âœ… cluster_info.tsv rows=567, cols=45\n",
      "âœ… template_metrics.tsv rows=567, cols=12\n",
      "ğŸ‰ Template metrics present in cluster_info (examples): ['exp_decay', 'half_width', 'num_negative_peaks', 'num_positive_peaks', 'peak_to_valley', 'peak_trough_ratio', 'recovery_slope', 'repolarization_slope']\n",
      "ğŸ”— Join keys â€” template_metrics unique=567, cluster_info unique=567\n",
      "\n",
      "=== Session: 20250925_M373_FourStimuli_g0 ===\n",
      "â†’ using output folder: 20250925_M373_FourStimuli_g0_output\n",
      " â€¢ [1/1] /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20250925_M373_FourStimuli_g0_output/preprocessed/block0_imec0.ap_recording1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home13/shliu/.conda/envs/spikeInterface/lib/python3.12/site-packages/spikeinterface/core/base.py:1109: UserWarning: Versions are not the same. This might lead to compatibility errors. Using spikeinterface==0.101.2 is recommended\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â†’ 1231 units\n",
      " âœ… Total units: 1231\n",
      "ğŸ¯ Wrote: spike_times.npy, spike_clusters.npy, cluster_group.tsv, cluster_info.tsv (+ template_metrics.tsv, best_channel_templates.npy)\n",
      "ğŸ§¾ Saved analysis_meta.json in /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20250925_M373_FourStimuli_g0_output/AIND_20250925_M373_FourStimuli_g0\n",
      "\n",
      "[diagnostics] AIND folder: /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20250925_M373_FourStimuli_g0_output/AIND_20250925_M373_FourStimuli_g0\n",
      "âœ… cluster_info.tsv rows=1231, cols=45\n",
      "âœ… template_metrics.tsv rows=1231, cols=12\n",
      "ğŸ‰ Template metrics present in cluster_info (examples): ['exp_decay', 'half_width', 'num_negative_peaks', 'num_positive_peaks', 'peak_to_valley', 'peak_trough_ratio', 'recovery_slope', 'repolarization_slope']\n",
      "ğŸ”— Join keys â€” template_metrics unique=1231, cluster_info unique=1231\n",
      "\n",
      "=== Session: 20251001_M381_4W50_g0 ===\n",
      "â†’ using output folder: 20251001_M381_4W50_g0_output\n",
      " â€¢ [1/1] /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20251001_M381_4W50_g0_output/preprocessed/block0_imec0.ap_recording1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home13/shliu/.conda/envs/spikeInterface/lib/python3.12/site-packages/spikeinterface/core/base.py:1109: UserWarning: Versions are not the same. This might lead to compatibility errors. Using spikeinterface==0.101.2 is recommended\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â†’ 536 units\n",
      " âœ… Total units: 536\n",
      "ğŸ¯ Wrote: spike_times.npy, spike_clusters.npy, cluster_group.tsv, cluster_info.tsv (+ template_metrics.tsv, best_channel_templates.npy)\n",
      "ğŸ§¾ Saved analysis_meta.json in /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20251001_M381_4W50_g0_output/AIND_20251001_M381_4W50_g0\n",
      "\n",
      "[diagnostics] AIND folder: /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20251001_M381_4W50_g0_output/AIND_20251001_M381_4W50_g0\n",
      "âœ… cluster_info.tsv rows=536, cols=45\n",
      "âœ… template_metrics.tsv rows=536, cols=12\n",
      "ğŸ‰ Template metrics present in cluster_info (examples): ['exp_decay', 'half_width', 'num_negative_peaks', 'num_positive_peaks', 'peak_to_valley', 'peak_trough_ratio', 'recovery_slope', 'repolarization_slope']\n",
      "ğŸ”— Join keys â€” template_metrics unique=536, cluster_info unique=536\n",
      "\n",
      "=== Session: 20251002_M380_4W50_g0 ===\n",
      "â†’ using output folder: 20251002_M380_4W50_g0_output\n",
      " â€¢ [1/1] /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20251002_M380_4W50_g0_output/preprocessed/block0_imec0.ap_recording1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home13/shliu/.conda/envs/spikeInterface/lib/python3.12/site-packages/spikeinterface/core/base.py:1109: UserWarning: Versions are not the same. This might lead to compatibility errors. Using spikeinterface==0.101.2 is recommended\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â†’ 639 units\n",
      " âœ… Total units: 639\n",
      "ğŸ¯ Wrote: spike_times.npy, spike_clusters.npy, cluster_group.tsv, cluster_info.tsv (+ template_metrics.tsv, best_channel_templates.npy)\n",
      "ğŸ§¾ Saved analysis_meta.json in /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20251002_M380_4W50_g0_output/AIND_20251002_M380_4W50_g0\n",
      "\n",
      "[diagnostics] AIND folder: /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20251002_M380_4W50_g0_output/AIND_20251002_M380_4W50_g0\n",
      "âœ… cluster_info.tsv rows=639, cols=45\n",
      "âœ… template_metrics.tsv rows=639, cols=12\n",
      "ğŸ‰ Template metrics present in cluster_info (examples): ['exp_decay', 'half_width', 'num_negative_peaks', 'num_positive_peaks', 'peak_to_valley', 'peak_trough_ratio', 'recovery_slope', 'repolarization_slope']\n",
      "ğŸ”— Join keys â€” template_metrics unique=639, cluster_info unique=639\n",
      "\n",
      "=== Session: 20251002_M381_4W50_g0 ===\n",
      "â†’ using output folder: 20251002_M381_4W50_g0_output\n",
      " â€¢ [1/1] /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20251002_M381_4W50_g0_output/preprocessed/block0_imec0.ap_recording1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home13/shliu/.conda/envs/spikeInterface/lib/python3.12/site-packages/spikeinterface/core/base.py:1109: UserWarning: Versions are not the same. This might lead to compatibility errors. Using spikeinterface==0.101.2 is recommended\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â†’ 493 units\n",
      " âœ… Total units: 493\n",
      "ğŸ¯ Wrote: spike_times.npy, spike_clusters.npy, cluster_group.tsv, cluster_info.tsv (+ template_metrics.tsv, best_channel_templates.npy)\n",
      "ğŸ§¾ Saved analysis_meta.json in /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20251002_M381_4W50_g0_output/AIND_20251002_M381_4W50_g0\n",
      "\n",
      "[diagnostics] AIND folder: /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20251002_M381_4W50_g0_output/AIND_20251002_M381_4W50_g0\n",
      "âœ… cluster_info.tsv rows=493, cols=45\n",
      "âœ… template_metrics.tsv rows=493, cols=12\n",
      "ğŸ‰ Template metrics present in cluster_info (examples): ['exp_decay', 'half_width', 'num_negative_peaks', 'num_positive_peaks', 'peak_to_valley', 'peak_trough_ratio', 'recovery_slope', 'repolarization_slope']\n",
      "ğŸ”— Join keys â€” template_metrics unique=493, cluster_info unique=493\n",
      "\n",
      "=== Session: 20251003_M380_4W50_g0 ===\n",
      "â†’ using output folder: 20251003_M380_4W50_g0_output\n",
      " â€¢ [1/1] /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20251003_M380_4W50_g0_output/preprocessed/block0_imec0.ap_recording1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home13/shliu/.conda/envs/spikeInterface/lib/python3.12/site-packages/spikeinterface/core/base.py:1109: UserWarning: Versions are not the same. This might lead to compatibility errors. Using spikeinterface==0.101.2 is recommended\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â†’ 562 units\n",
      " âœ… Total units: 562\n",
      "ğŸ¯ Wrote: spike_times.npy, spike_clusters.npy, cluster_group.tsv, cluster_info.tsv (+ template_metrics.tsv, best_channel_templates.npy)\n",
      "ğŸ§¾ Saved analysis_meta.json in /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20251003_M380_4W50_g0_output/AIND_20251003_M380_4W50_g0\n",
      "\n",
      "[diagnostics] AIND folder: /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20251003_M380_4W50_g0_output/AIND_20251003_M380_4W50_g0\n",
      "âœ… cluster_info.tsv rows=562, cols=45\n",
      "âœ… template_metrics.tsv rows=562, cols=12\n",
      "ğŸ‰ Template metrics present in cluster_info (examples): ['exp_decay', 'half_width', 'num_negative_peaks', 'num_positive_peaks', 'peak_to_valley', 'peak_trough_ratio', 'recovery_slope', 'repolarization_slope']\n",
      "ğŸ”— Join keys â€” template_metrics unique=562, cluster_info unique=562\n",
      "\n",
      "=== Session: 20251003_M381_4W50_g0 ===\n",
      "â†’ using output folder: 20251003_M381_4W50_g0_output\n",
      " â€¢ [1/1] /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20251003_M381_4W50_g0_output/preprocessed/block0_imec0.ap_recording1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home13/shliu/.conda/envs/spikeInterface/lib/python3.12/site-packages/spikeinterface/core/base.py:1109: UserWarning: Versions are not the same. This might lead to compatibility errors. Using spikeinterface==0.101.2 is recommended\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â†’ 513 units\n",
      " âœ… Total units: 513\n",
      "ğŸ¯ Wrote: spike_times.npy, spike_clusters.npy, cluster_group.tsv, cluster_info.tsv (+ template_metrics.tsv, best_channel_templates.npy)\n",
      "ğŸ§¾ Saved analysis_meta.json in /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20251003_M381_4W50_g0_output/AIND_20251003_M381_4W50_g0\n",
      "\n",
      "[diagnostics] AIND folder: /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20251003_M381_4W50_g0_output/AIND_20251003_M381_4W50_g0\n",
      "âœ… cluster_info.tsv rows=513, cols=45\n",
      "âœ… template_metrics.tsv rows=513, cols=12\n",
      "ğŸ‰ Template metrics present in cluster_info (examples): ['exp_decay', 'half_width', 'num_negative_peaks', 'num_positive_peaks', 'peak_to_valley', 'peak_trough_ratio', 'recovery_slope', 'repolarization_slope']\n",
      "ğŸ”— Join keys â€” template_metrics unique=513, cluster_info unique=513\n",
      "\n",
      "=== Session: 20251004_M380_4W50_g0 ===\n",
      "â†’ using output folder: 20251004_M380_4W50_g0_output\n",
      " â€¢ [1/1] /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20251004_M380_4W50_g0_output/preprocessed/block0_imec0.ap_recording1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home13/shliu/.conda/envs/spikeInterface/lib/python3.12/site-packages/spikeinterface/core/base.py:1109: UserWarning: Versions are not the same. This might lead to compatibility errors. Using spikeinterface==0.101.2 is recommended\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â†’ 698 units\n",
      " âœ… Total units: 698\n",
      "ğŸ¯ Wrote: spike_times.npy, spike_clusters.npy, cluster_group.tsv, cluster_info.tsv (+ template_metrics.tsv, best_channel_templates.npy)\n",
      "ğŸ§¾ Saved analysis_meta.json in /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20251004_M380_4W50_g0_output/AIND_20251004_M380_4W50_g0\n",
      "\n",
      "[diagnostics] AIND folder: /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20251004_M380_4W50_g0_output/AIND_20251004_M380_4W50_g0\n",
      "âœ… cluster_info.tsv rows=698, cols=45\n",
      "âœ… template_metrics.tsv rows=698, cols=12\n",
      "ğŸ‰ Template metrics present in cluster_info (examples): ['exp_decay', 'half_width', 'num_negative_peaks', 'num_positive_peaks', 'peak_to_valley', 'peak_trough_ratio', 'recovery_slope', 'repolarization_slope']\n",
      "ğŸ”— Join keys â€” template_metrics unique=698, cluster_info unique=698\n",
      "\n",
      "=== Session: 20251004_M381_4W50_g0 ===\n",
      "â†’ using output folder: 20251004_M381_4W50_g0_output\n",
      " â€¢ [1/1] /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20251004_M381_4W50_g0_output/preprocessed/block0_imec0.ap_recording1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home13/shliu/.conda/envs/spikeInterface/lib/python3.12/site-packages/spikeinterface/core/base.py:1109: UserWarning: Versions are not the same. This might lead to compatibility errors. Using spikeinterface==0.101.2 is recommended\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â†’ 544 units\n",
      " âœ… Total units: 544\n",
      "ğŸ¯ Wrote: spike_times.npy, spike_clusters.npy, cluster_group.tsv, cluster_info.tsv (+ template_metrics.tsv, best_channel_templates.npy)\n",
      "ğŸ§¾ Saved analysis_meta.json in /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20251004_M381_4W50_g0_output/AIND_20251004_M381_4W50_g0\n",
      "\n",
      "[diagnostics] AIND folder: /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20251004_M381_4W50_g0_output/AIND_20251004_M381_4W50_g0\n",
      "âœ… cluster_info.tsv rows=544, cols=45\n",
      "âœ… template_metrics.tsv rows=544, cols=12\n",
      "ğŸ‰ Template metrics present in cluster_info (examples): ['exp_decay', 'half_width', 'num_negative_peaks', 'num_positive_peaks', 'peak_to_valley', 'peak_trough_ratio', 'recovery_slope', 'repolarization_slope']\n",
      "ğŸ”— Join keys â€” template_metrics unique=544, cluster_info unique=544\n",
      "\n",
      "=== Session: 20251005_M380_4W50_g0 ===\n",
      "â†’ using output folder: 20251005_M380_4W50_g0_output\n",
      " â€¢ [1/1] /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20251005_M380_4W50_g0_output/preprocessed/block0_imec0.ap_recording1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home13/shliu/.conda/envs/spikeInterface/lib/python3.12/site-packages/spikeinterface/core/base.py:1109: UserWarning: Versions are not the same. This might lead to compatibility errors. Using spikeinterface==0.101.2 is recommended\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â†’ 572 units\n",
      " âœ… Total units: 572\n",
      "ğŸ¯ Wrote: spike_times.npy, spike_clusters.npy, cluster_group.tsv, cluster_info.tsv (+ template_metrics.tsv, best_channel_templates.npy)\n",
      "ğŸ§¾ Saved analysis_meta.json in /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20251005_M380_4W50_g0_output/AIND_20251005_M380_4W50_g0\n",
      "\n",
      "[diagnostics] AIND folder: /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20251005_M380_4W50_g0_output/AIND_20251005_M380_4W50_g0\n",
      "âœ… cluster_info.tsv rows=572, cols=45\n",
      "âœ… template_metrics.tsv rows=572, cols=12\n",
      "ğŸ‰ Template metrics present in cluster_info (examples): ['exp_decay', 'half_width', 'num_negative_peaks', 'num_positive_peaks', 'peak_to_valley', 'peak_trough_ratio', 'recovery_slope', 'repolarization_slope']\n",
      "ğŸ”— Join keys â€” template_metrics unique=572, cluster_info unique=572\n",
      "\n",
      "=== Session: 20251005_M381_4W50_g0 ===\n",
      "â†’ using output folder: 20251005_M381_4W50_g0_output\n",
      " â€¢ [1/1] /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20251005_M381_4W50_g0_output/preprocessed/block0_imec0.ap_recording1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home13/shliu/.conda/envs/spikeInterface/lib/python3.12/site-packages/spikeinterface/core/base.py:1109: UserWarning: Versions are not the same. This might lead to compatibility errors. Using spikeinterface==0.101.2 is recommended\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â†’ 494 units\n",
      " âœ… Total units: 494\n",
      "ğŸ¯ Wrote: spike_times.npy, spike_clusters.npy, cluster_group.tsv, cluster_info.tsv (+ template_metrics.tsv, best_channel_templates.npy)\n",
      "ğŸ§¾ Saved analysis_meta.json in /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20251005_M381_4W50_g0_output/AIND_20251005_M381_4W50_g0\n",
      "\n",
      "[diagnostics] AIND folder: /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20251005_M381_4W50_g0_output/AIND_20251005_M381_4W50_g0\n",
      "âœ… cluster_info.tsv rows=494, cols=45\n",
      "âœ… template_metrics.tsv rows=494, cols=12\n",
      "ğŸ‰ Template metrics present in cluster_info (examples): ['exp_decay', 'half_width', 'num_negative_peaks', 'num_positive_peaks', 'peak_to_valley', 'peak_trough_ratio', 'recovery_slope', 'repolarization_slope']\n",
      "ğŸ”— Join keys â€” template_metrics unique=494, cluster_info unique=494\n",
      "\n",
      "=== Session: 20251006_M380_4W50LickNumBlock_g0 ===\n",
      "â†’ using output folder: 20251006_M380_4W50LickNumBlock_g0_output\n",
      " â€¢ [1/1] /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20251006_M380_4W50LickNumBlock_g0_output/preprocessed/block0_imec0.ap_recording1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home13/shliu/.conda/envs/spikeInterface/lib/python3.12/site-packages/spikeinterface/core/base.py:1109: UserWarning: Versions are not the same. This might lead to compatibility errors. Using spikeinterface==0.101.2 is recommended\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â†’ 684 units\n",
      " âœ… Total units: 684\n",
      "ğŸ¯ Wrote: spike_times.npy, spike_clusters.npy, cluster_group.tsv, cluster_info.tsv (+ template_metrics.tsv, best_channel_templates.npy)\n",
      "ğŸ§¾ Saved analysis_meta.json in /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20251006_M380_4W50LickNumBlock_g0_output/AIND_20251006_M380_4W50LickNumBlock_g0\n",
      "\n",
      "[diagnostics] AIND folder: /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20251006_M380_4W50LickNumBlock_g0_output/AIND_20251006_M380_4W50LickNumBlock_g0\n",
      "âœ… cluster_info.tsv rows=684, cols=45\n",
      "âœ… template_metrics.tsv rows=684, cols=12\n",
      "ğŸ‰ Template metrics present in cluster_info (examples): ['exp_decay', 'half_width', 'num_negative_peaks', 'num_positive_peaks', 'peak_to_valley', 'peak_trough_ratio', 'recovery_slope', 'repolarization_slope']\n",
      "ğŸ”— Join keys â€” template_metrics unique=684, cluster_info unique=684\n",
      "\n",
      "=== Session: 20251006_M381_4W50LickNumBlock_g0 ===\n",
      "â†’ using output folder: 20251006_M381_4W50LickNumBlock_g0_output\n",
      " â€¢ [1/1] /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20251006_M381_4W50LickNumBlock_g0_output/preprocessed/block0_imec0.ap_recording1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home13/shliu/.conda/envs/spikeInterface/lib/python3.12/site-packages/spikeinterface/core/base.py:1109: UserWarning: Versions are not the same. This might lead to compatibility errors. Using spikeinterface==0.101.2 is recommended\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â†’ 526 units\n",
      " âœ… Total units: 526\n",
      "ğŸ¯ Wrote: spike_times.npy, spike_clusters.npy, cluster_group.tsv, cluster_info.tsv (+ template_metrics.tsv, best_channel_templates.npy)\n",
      "ğŸ§¾ Saved analysis_meta.json in /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20251006_M381_4W50LickNumBlock_g0_output/AIND_20251006_M381_4W50LickNumBlock_g0\n",
      "\n",
      "[diagnostics] AIND folder: /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20251006_M381_4W50LickNumBlock_g0_output/AIND_20251006_M381_4W50LickNumBlock_g0\n",
      "âœ… cluster_info.tsv rows=526, cols=45\n",
      "âœ… template_metrics.tsv rows=526, cols=12\n",
      "ğŸ‰ Template metrics present in cluster_info (examples): ['exp_decay', 'half_width', 'num_negative_peaks', 'num_positive_peaks', 'peak_to_valley', 'peak_trough_ratio', 'recovery_slope', 'repolarization_slope']\n",
      "ğŸ”— Join keys â€” template_metrics unique=526, cluster_info unique=526\n",
      "\n",
      "=== Session: 20251006_M381_4W50_DCZ300_g0 ===\n",
      "â†’ using output folder: 20251006_M381_4W50_DCZ300_g0_output\n",
      " â€¢ [1/1] /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20251006_M381_4W50_DCZ300_g0_output/preprocessed/block0_imec0.ap_recording1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home13/shliu/.conda/envs/spikeInterface/lib/python3.12/site-packages/spikeinterface/core/base.py:1109: UserWarning: Versions are not the same. This might lead to compatibility errors. Using spikeinterface==0.101.2 is recommended\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â†’ 467 units\n",
      " âœ… Total units: 467\n",
      "ğŸ¯ Wrote: spike_times.npy, spike_clusters.npy, cluster_group.tsv, cluster_info.tsv (+ template_metrics.tsv, best_channel_templates.npy)\n",
      "ğŸ§¾ Saved analysis_meta.json in /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20251006_M381_4W50_DCZ300_g0_output/AIND_20251006_M381_4W50_DCZ300_g0\n",
      "\n",
      "[diagnostics] AIND folder: /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch/20251006_M381_4W50_DCZ300_g0_output/AIND_20251006_M381_4W50_DCZ300_g0\n",
      "âœ… cluster_info.tsv rows=467, cols=45\n",
      "âœ… template_metrics.tsv rows=467, cols=12\n",
      "ğŸ‰ Template metrics present in cluster_info (examples): ['exp_decay', 'half_width', 'num_negative_peaks', 'num_positive_peaks', 'peak_to_valley', 'peak_trough_ratio', 'recovery_slope', 'repolarization_slope']\n",
      "ğŸ”— Join keys â€” template_metrics unique=467, cluster_info unique=467\n"
     ]
    }
   ],
   "source": [
    "# === Shijia AIND export with template metrics merge + best-channel templates + T2P + analysis_meta + diagnostics ===\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import time\n",
    "import platform\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spikeinterface as si\n",
    "\n",
    "# -------- Base folder paths --------\n",
    "# input_base_dir  = '/n/netscratch/bsabatini_lab/Lab/shliu/imec0/aind_input/todo'\n",
    "input_base_dir  = '/n/netscratch/bsabatini_lab/Lab/shliu/imec0/aind_input'\n",
    "output_base_dir = '/n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch'\n",
    "\n",
    "# -------- Discover raw-recording subfolders and derive session names --------\n",
    "all_raw_folders = []\n",
    "session_names   = []\n",
    "\n",
    "for item in sorted(os.listdir(input_base_dir)):\n",
    "    full_path = os.path.join(input_base_dir, item)\n",
    "    if not os.path.isdir(full_path):\n",
    "        continue\n",
    "\n",
    "    if item.endswith('_imec0') or item.endswith('_imec1'):\n",
    "        all_raw_folders.append(full_path)\n",
    "        session_names.append('_'.join(item.split('_')[:-1]))\n",
    "    else:\n",
    "        for sd in sorted(os.listdir(full_path)):\n",
    "            if ('imec0' in sd or 'imec1' in sd) and os.path.isdir(os.path.join(full_path, sd)):\n",
    "                all_raw_folders.append(os.path.join(full_path, sd))\n",
    "                session_names.append(item)\n",
    "\n",
    "# -------- Helpers --------\n",
    "def find_experiment_files(preproc_path):\n",
    "    files = glob.glob(os.path.join(preproc_path, 'block0_imec*.ap_recording1*.json'))\n",
    "    files.sort()\n",
    "    return files\n",
    "\n",
    "def _parse_ap_meta(meta_path):\n",
    "    out_raw = {}\n",
    "    if meta_path and os.path.exists(meta_path):\n",
    "        with open(meta_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if \"=\" in line:\n",
    "                    k, v = line.split(\"=\", 1)\n",
    "                    out_raw[k.strip()] = v.strip()\n",
    "    out_sum = {}\n",
    "    if \"imSampRate\" in out_raw:\n",
    "        try: out_sum[\"sampling_rate_hz\"] = float(out_raw[\"imSampRate\"])\n",
    "        except: pass\n",
    "    if \"nSavedChans\" in out_raw:\n",
    "        try: out_sum[\"n_saved_channels\"] = int(out_raw[\"nSavedChans\"])\n",
    "        except: pass\n",
    "    if \"fileTimeSecs\" in out_raw:\n",
    "        try: out_sum[\"file_time_secs\"] = float(out_raw[\"fileTimeSecs\"])\n",
    "        except: pass\n",
    "    if \"fileName\" in out_raw:\n",
    "        out_sum[\"ap_bin_path\"] = out_raw[\"fileName\"]\n",
    "    return {\"raw_meta\": out_raw, \"summary\": out_sum}\n",
    "\n",
    "def check_template_metrics_merge(aind_folder):\n",
    "    tsv_tm  = os.path.join(aind_folder, \"template_metrics.tsv\")\n",
    "    tsv_ci  = os.path.join(aind_folder, \"cluster_info.tsv\")\n",
    "\n",
    "    print(f\"\\n[diagnostics] AIND folder: {aind_folder}\")\n",
    "\n",
    "    if not os.path.exists(tsv_ci):\n",
    "        print(\"âŒ cluster_info.tsv not found.\")\n",
    "        return\n",
    "    ci = pd.read_csv(tsv_ci, sep=\"\\t\")\n",
    "    print(f\"âœ… cluster_info.tsv rows={len(ci)}, cols={len(ci.columns)}\")\n",
    "\n",
    "    if not os.path.exists(tsv_tm):\n",
    "        print(\"âš ï¸ template_metrics.tsv not found (merge likely skipped).\")\n",
    "        candidate_cols = [c for c in ci.columns if any(k in c.lower() for k in\n",
    "                             [\"trough\", \"peak\", \"half\", \"width\", \"ptp\", \"duration\", \"repolar\"])]\n",
    "        if candidate_cols:\n",
    "            print(f\"   â€¢ Found template-like columns in cluster_info: {candidate_cols[:8]}\")\n",
    "        else:\n",
    "            print(\"   â€¢ No template-like columns detected in cluster_info.\")\n",
    "        return\n",
    "\n",
    "    tm = pd.read_csv(tsv_tm, sep=\"\\t\")\n",
    "    print(f\"âœ… template_metrics.tsv rows={len(tm)}, cols={len(tm.columns)}\")\n",
    "    key_cols = {\"global_unit_ids\", \"unit_id\", \"unit_ids\"}\n",
    "    tm_metric_cols = [c for c in tm.columns if c not in key_cols]\n",
    "    present = [c for c in tm_metric_cols if c in ci.columns]\n",
    "    missing = [c for c in tm_metric_cols if c not in ci.columns]\n",
    "    if present:\n",
    "        print(f\"ğŸ‰ Template metrics present in cluster_info (examples): {present[:8]}\")\n",
    "    else:\n",
    "        print(\"âŒ No template-metric columns found in cluster_info.\")\n",
    "    if missing:\n",
    "        print(f\"â„¹ï¸ Some template-metric columns not found in cluster_info (first few): {missing[:8]}\")\n",
    "    key_in_tm  = \"global_unit_ids\" if \"global_unit_ids\" in tm.columns else (\"unit_id\" if \"unit_id\" in tm.columns else None)\n",
    "    key_in_ci  = \"global_unit_ids\" if \"global_unit_ids\" in ci.columns else None\n",
    "    if key_in_tm and key_in_ci:\n",
    "        covered = tm[key_in_tm].nunique()\n",
    "        in_ci   = ci[key_in_ci].nunique()\n",
    "        print(f\"ğŸ”— Join keys â€” template_metrics unique={covered}, cluster_info unique={in_ci}\")\n",
    "    else:\n",
    "        print(f\"âš ï¸ Could not compare join keys (key_in_tm={key_in_tm}, key_in_ci={key_in_ci})\")\n",
    "\n",
    "# --- internal helpers for templates access and T2P ---\n",
    "def _fetch_templates_array(sa):\n",
    "    \"\"\"\n",
    "    Try several access paths across SI 0.102.x.\n",
    "    Return a numpy array with shape (n_units, n_samples, n_channels) if possible,\n",
    "    or (n_units, n_channels, n_samples). Also return string axis_mode: 'ucs' or 'usc'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        sa.load_extension('templates')\n",
    "    except Exception:\n",
    "        sa.compute(['templates'], n_jobs=4, progress_bar=False)\n",
    "    ext = sa.get_extension('templates')\n",
    "    arr = None\n",
    "    # 1) get_data -> dict['templates']\n",
    "    try:\n",
    "        data = ext.get_data()\n",
    "        if isinstance(data, dict) and isinstance(data.get('templates', None), np.ndarray):\n",
    "            arr = data['templates']\n",
    "    except Exception:\n",
    "        pass\n",
    "    # 2) get_all_templates / get_templates\n",
    "    if arr is None and hasattr(ext, 'get_all_templates'):\n",
    "        try: arr = ext.get_all_templates()\n",
    "        except Exception: pass\n",
    "    if arr is None and hasattr(ext, 'get_templates'):\n",
    "        try: arr = ext.get_templates()\n",
    "        except Exception: pass\n",
    "    if not (isinstance(arr, np.ndarray) and arr.ndim == 3):\n",
    "        return None, None\n",
    "    # decide axis order\n",
    "    # (units, ?, ?) where one axis is samples (usually >= 30), the other is channels (<= 512)\n",
    "    a1, a2 = arr.shape[1], arr.shape[2]\n",
    "    if a1 <= 512 and a2 > 32:\n",
    "        return arr, 'usc'  # (units, channels, samples)\n",
    "    else:\n",
    "        return arr, 'ucs'  # (units, samples, channels)\n",
    "\n",
    "def _ptp_per_channel(template_unit, axis_mode):\n",
    "    \"\"\"Return per-channel peak-to-peak amplitude vector for one unit template.\"\"\"\n",
    "    if axis_mode == 'ucs':   # (samples, channels)\n",
    "        # template_unit shape: (n_samples, n_channels)\n",
    "        return (template_unit.max(axis=0) - template_unit.min(axis=0))\n",
    "    else:                     # 'usc' -> (channels, samples)\n",
    "        return (template_unit.max(axis=1) - template_unit.min(axis=1))\n",
    "\n",
    "def _extract_best_channel_waveform(templates_arr, unit_index, best_ch, axis_mode):\n",
    "    if axis_mode == 'ucs':   # (units, samples, channels)\n",
    "        return templates_arr[unit_index, :, best_ch]\n",
    "    else:                     # (units, channels, samples)\n",
    "        return templates_arr[unit_index, best_ch, :]\n",
    "\n",
    "def _trough_to_peak_ms(wf_1d, fs_hz):\n",
    "    \"\"\"Compute trough-to-peak latency (ms) on a 1-D waveform.\"\"\"\n",
    "    if wf_1d is None or len(wf_1d) < 3:\n",
    "        return np.nan\n",
    "    i_trough = int(np.argmin(wf_1d))\n",
    "    # peak after trough; if none, use absolute peak\n",
    "    if i_trough < len(wf_1d) - 1:\n",
    "        i_peak_rel = int(np.argmax(wf_1d[i_trough:]))\n",
    "        i_peak = i_trough + i_peak_rel\n",
    "    else:\n",
    "        i_peak = int(np.argmax(wf_1d))\n",
    "    dt_samples = max(i_peak - i_trough, 0)\n",
    "    return (dt_samples / float(fs_hz)) * 1000.0\n",
    "\n",
    "# -------- Main processing loop --------\n",
    "for raw_rec, session_name in zip(all_raw_folders, session_names):\n",
    "    print(f\"\\n=== Session: {session_name} ===\")\n",
    "\n",
    "    # match session output folder\n",
    "    matches = [d for d in os.listdir(output_base_dir)\n",
    "               if session_name in d \n",
    "               and os.path.isdir(os.path.join(output_base_dir, d))]\n",
    "    if not matches:\n",
    "        print(f\"âš ï¸  No output folder found for '{session_name}' in {output_base_dir}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    output_folder = sorted(matches)[0]\n",
    "    baseFolder    = os.path.join(output_base_dir, output_folder)\n",
    "    print(f\"â†’ using output folder: {output_folder}\")\n",
    "\n",
    "    # Prepare AIND output dir\n",
    "    AIND_folder = os.path.join(baseFolder, f'AIND_{session_name}')\n",
    "    os.makedirs(AIND_folder, exist_ok=True)\n",
    "\n",
    "    # define subfolders\n",
    "    preProcessed = os.path.join(baseFolder, 'preprocessed')\n",
    "    postProcessed= os.path.join(baseFolder, 'postprocessed')\n",
    "    spikes       = os.path.join(baseFolder, 'spikesorted')\n",
    "    curated      = os.path.join(baseFolder, 'curated')\n",
    "\n",
    "    experiment_files = find_experiment_files(preProcessed)\n",
    "    if not experiment_files:\n",
    "        print(\"âš ï¸  No experiment JSONs found under:\", preProcessed)\n",
    "        continue\n",
    "\n",
    "    # accumulators\n",
    "    total_units = 0\n",
    "    all_spike_times    = []\n",
    "    all_spike_clusters = []\n",
    "    unit_labels_combined      = []\n",
    "    qm_combined_with_global_ids = []\n",
    "    tm_all_segments = []       # template metrics per segment (GLOBAL-keyed)\n",
    "    uloc_all_segments = []     # unit locations per segment (GLOBAL-keyed, optional)\n",
    "\n",
    "    # best-channel outputs\n",
    "    best_templates_all = []    # list of 1-D arrays (best-channel template)\n",
    "    best_templates_meta = []   # dicts with global_unit_ids, peak_channel, segment_name, n_samples, trough_to_peak_ms\n",
    "\n",
    "    global_unit_counter = 1\n",
    "\n",
    "    for i, expf in enumerate(experiment_files, start=1):\n",
    "        print(f\" â€¢ [{i}/{len(experiment_files)}] {expf}\")\n",
    "        exp_base = os.path.splitext(os.path.basename(expf))[0]\n",
    "        zarr_path = os.path.join(postProcessed, exp_base + '.zarr')\n",
    "        if not os.path.exists(zarr_path):\n",
    "            print(\"   â€“ Missing zarr:\", zarr_path)\n",
    "            continue\n",
    "\n",
    "        # load sorting & spikes with SI 0.102.x API\n",
    "        sorting_analyzer = si.load_sorting_analyzer(zarr_path, load_extensions=False)\n",
    "        sorting_curated  = si.load(os.path.join(curated, exp_base))\n",
    "        spike_extractor  = si.load(os.path.join(spikes,   exp_base))\n",
    "\n",
    "        unit_ids = sorting_curated.get_unit_ids()\n",
    "        labels   = sorting_curated.get_property('decoder_label')\n",
    "        fs_hz    = sorting_curated.get_sampling_frequency()\n",
    "        print(f\"   â†’ {len(unit_ids)} units\")\n",
    "\n",
    "        # quality metrics\n",
    "        sorting_analyzer.load_extension('quality_metrics')\n",
    "        qm = sorting_analyzer.get_extension('quality_metrics').get_data()\n",
    "        qm_df = pd.DataFrame(qm)\n",
    "        if \"unit_ids\" not in qm_df.columns:\n",
    "            qm_df['unit_ids'] = unit_ids  # LOCAL IDs (keep this column)\n",
    "\n",
    "        # From Tom, added 20250911: add unit locations (x, y in Âµm; typically x = lateral, y = depth)\n",
    "        sorting_analyzer.load_extension('unit_locations')\n",
    "        unit_locs = sorting_analyzer.get_extension('unit_locations').get_data()  # shape (n_units, 2) or (n_units, 3)\n",
    "        try:\n",
    "            # common case: numpy array\n",
    "            qm_df['x_um'] = unit_locs[:, 0]\n",
    "            qm_df['y_um'] = unit_locs[:, 1]\n",
    "        except Exception:\n",
    "            # fallback if SI returns a DataFrame-like\n",
    "            uloc_df = pd.DataFrame(unit_locs)\n",
    "            if 'x_um' in uloc_df.columns and 'y_um' in uloc_df.columns:\n",
    "                qm_df['x_um'] = uloc_df['x_um'].to_numpy()\n",
    "                qm_df['y_um'] = uloc_df['y_um'].to_numpy()\n",
    "            else:\n",
    "                # last resort: 'x'/'y' names\n",
    "                qm_df['x_um'] = uloc_df['x'].to_numpy()\n",
    "                qm_df['y_um'] = uloc_df['y'].to_numpy()\n",
    "            \n",
    "        # unit locations (Âµm) â€” optional\n",
    "        try:\n",
    "            sorting_analyzer.load_extension('unit_locations')\n",
    "            uloc = sorting_analyzer.get_extension('unit_locations').get_data()\n",
    "            uloc_df = pd.DataFrame(uloc)\n",
    "            if 'unit_id' not in uloc_df.columns:\n",
    "                uloc_df = uloc_df.reset_index().rename(columns={'index':'unit_id'})\n",
    "            uloc_df['unit_id'] = uloc_df['unit_id'].astype(int)\n",
    "        except Exception as e:\n",
    "            uloc_df = None\n",
    "            print(f\"   (info) unit_locations not found: {e}\")\n",
    "\n",
    "        # LOCAL â†’ GLOBAL mapping for this block\n",
    "        map_df = pd.DataFrame({\n",
    "            'unit_ids': unit_ids,\n",
    "            'global_unit_ids': np.arange(global_unit_counter, global_unit_counter + len(unit_ids))\n",
    "        })\n",
    "\n",
    "        # attach global ids to QC rows\n",
    "        qm_df = qm_df.merge(map_df, on='unit_ids', how='left')\n",
    "        qm_combined_with_global_ids.append(qm_df)\n",
    "\n",
    "        # labels table\n",
    "        unit_labels_combined.append(pd.DataFrame({\n",
    "            'global_unit_ids': map_df['global_unit_ids'],\n",
    "            'labels': labels\n",
    "        }))\n",
    "\n",
    "        # spikes (sample indices) per local unit, tagged with GLOBAL id\n",
    "        for local_id, global_id in zip(map_df['unit_ids'], map_df['global_unit_ids']):\n",
    "            try:\n",
    "                stimes = spike_extractor.get_unit_spike_train(local_id)  # samples\n",
    "                all_spike_times.extend(stimes)\n",
    "                all_spike_clusters.extend([global_id] * len(stimes))\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "        # ---- per-segment TEMPLATE METRICS (Tom-style) ----\n",
    "        try:\n",
    "            try:\n",
    "                sorting_analyzer.load_extension('template_metrics')\n",
    "            except Exception:\n",
    "                sorting_analyzer.compute(['templates', 'template_metrics'], n_jobs=4, progress_bar=False)\n",
    "            tm_seg = sorting_analyzer.get_extension('template_metrics').get_data()\n",
    "            tm_seg_df = pd.DataFrame(tm_seg)\n",
    "            if 'unit_id' not in tm_seg_df.columns:\n",
    "                tm_seg_df = tm_seg_df.reset_index().rename(columns={'index': 'unit_id'})\n",
    "            tm_seg_df['unit_id'] = tm_seg_df['unit_id'].astype(int)\n",
    "            # map LOCAL -> GLOBAL; keep ONLY global key here\n",
    "            tm_seg_df = tm_seg_df.merge(map_df.rename(columns={'unit_ids':'unit_id'}),\n",
    "                                        on='unit_id', how='left')\n",
    "            tm_seg_df.drop(columns=['unit_id'], inplace=True)\n",
    "            tm_all_segments.append(tm_seg_df)\n",
    "        except Exception as e:\n",
    "            print(f\"   (info) template_metrics unavailable for this segment: {e}\")\n",
    "            tm_seg_df = None  # important for best-channel fallback below\n",
    "\n",
    "        # ---- per-segment unit_locations mapped to GLOBAL (optional) ----\n",
    "        if uloc_df is not None:\n",
    "            uloc_df = uloc_df.merge(map_df.rename(columns={'unit_ids':'unit_id'}), on='unit_id', how='left')\n",
    "            keep_cols = ['global_unit_ids']\n",
    "            for col in ['x','y','x_um','y_um','z','z_um']:\n",
    "                if col in uloc_df.columns: keep_cols.append(col)\n",
    "            uloc_all_segments.append(uloc_df[keep_cols])\n",
    "\n",
    "        # === BEST-CHANNEL TEMPLATES + T2P per segment ===\n",
    "        # 1) Fetch templates array\n",
    "        templates_arr, axis_mode = _fetch_templates_array(sorting_analyzer)\n",
    "        if templates_arr is None:\n",
    "            print(\"   (info) templates array not available; skipping best-channel export for this segment.\")\n",
    "        else:\n",
    "            # 2) Determine best/peak channel per LOCAL unit\n",
    "            peak_col = None\n",
    "            if tm_seg_df is not None:\n",
    "                for cand in ('peak_channel', 'best_channel', 'max_ptp_channel'):\n",
    "                    if cand in tm_seg_df.columns:\n",
    "                        peak_col = cand\n",
    "                        break\n",
    "\n",
    "            # Build LOCAL -> GLOBAL map with peak channel if present\n",
    "            local_ids = pd.Series(unit_ids, name='unit_id')\n",
    "            map_local = local_ids.to_frame().merge(\n",
    "                map_df.rename(columns={'unit_ids':'unit_id'}), on='unit_id', how='left'\n",
    "            )\n",
    "\n",
    "            if peak_col is not None:\n",
    "                # Recover peak channel per GLOBAL from tm_seg_df\n",
    "                # (tm_seg_df currently only has global key; re-merge the original tm with local ids)\n",
    "                try:\n",
    "                    sorting_analyzer.load_extension('template_metrics')\n",
    "                    tm_full = sorting_analyzer.get_extension('template_metrics').get_data()\n",
    "                    tm_full = pd.DataFrame(tm_full)\n",
    "                    if 'unit_id' not in tm_full.columns:\n",
    "                        tm_full = tm_full.reset_index().rename(columns={'index':'unit_id'})\n",
    "                    tm_full['unit_id'] = tm_full['unit_id'].astype(int)\n",
    "                    map_local = map_local.merge(tm_full[['unit_id', peak_col]], on='unit_id', how='left')\n",
    "                except Exception:\n",
    "                    peak_col = None  # fall back to compute from PTP\n",
    "                    print(\"   (info) peak channel column not retrievable from extension; falling back to PTP argmax.\")\n",
    "\n",
    "            # If no peak_col information, compute channel PTP and pick argmax\n",
    "            if peak_col is None:\n",
    "                # templates_arr shape: (units, S, C) or (units, C, S)\n",
    "                # We assume the unit order follows sorting_curated.get_unit_ids()\n",
    "                for u_idx, u_local in enumerate(unit_ids):\n",
    "                    if axis_mode == 'ucs':\n",
    "                        unit_template = templates_arr[u_idx, :, :]   # (S, C)\n",
    "                    else:\n",
    "                        unit_template = templates_arr[u_idx, :, :]   # (C, S)\n",
    "                    ptp_vec = _ptp_per_channel(unit_template, axis_mode)\n",
    "                    best_ch = int(np.argmax(ptp_vec))\n",
    "                    map_local.loc[map_local['unit_id'] == u_local, 'peak_channel'] = best_ch\n",
    "\n",
    "            # 3) Extract best-channel 1-D waveform + compute T2P, n_samples\n",
    "            # Create lookup from LOCAL unit -> (GLOBAL id, peak_channel)\n",
    "            local_to_global = dict(zip(map_local['unit_id'].tolist(), map_local['global_unit_ids'].tolist()))\n",
    "            local_to_peak   = dict(zip(map_local['unit_id'].tolist(), map_local['peak_channel'].astype(int).tolist()))\n",
    "\n",
    "            # n_samples from templates\n",
    "            if axis_mode == 'ucs':\n",
    "                n_samples_seg = int(templates_arr.shape[1])\n",
    "            else:\n",
    "                n_samples_seg = int(templates_arr.shape[2])\n",
    "\n",
    "            for u_idx, u_local in enumerate(unit_ids):\n",
    "                g_uid = int(local_to_global.get(u_local, -1))\n",
    "                if g_uid < 0:  # skip unmapped\n",
    "                    continue\n",
    "                best_ch = int(local_to_peak.get(u_local, 0))\n",
    "                wf_1d = _extract_best_channel_waveform(templates_arr, u_idx, best_ch, axis_mode).astype(np.float32)\n",
    "                t2p_ms = _trough_to_peak_ms(wf_1d, fs_hz)\n",
    "\n",
    "                best_templates_all.append(wf_1d)\n",
    "                best_templates_meta.append({\n",
    "                    'global_unit_ids': g_uid,\n",
    "                    'peak_channel': best_ch,\n",
    "                    'segment_name': exp_base,\n",
    "                    'n_samples': n_samples_seg,\n",
    "                    'trough_to_peak_ms': float(t2p_ms),\n",
    "                })\n",
    "\n",
    "        global_unit_counter += len(unit_ids)\n",
    "        total_units += len(unit_ids)\n",
    "\n",
    "    print(f\" âœ… Total units: {total_units}\")\n",
    "\n",
    "    # -------- Concatenate per-session tables --------\n",
    "    unit_labels_df = (pd.concat(unit_labels_combined, ignore_index=True)\n",
    "                      if unit_labels_combined else pd.DataFrame())\n",
    "    qm_combined_df = (pd.concat(qm_combined_with_global_ids, ignore_index=True)\n",
    "                      if qm_combined_with_global_ids else pd.DataFrame())\n",
    "\n",
    "    # -------- Save the three core artifacts --------\n",
    "    spike_times    = np.array(all_spike_times)      # samples\n",
    "    spike_clusters = np.array(all_spike_clusters)   # GLOBAL unit ids\n",
    "    sort_idx       = np.argsort(spike_times)\n",
    "    spike_times    = spike_times[sort_idx]\n",
    "    spike_clusters = spike_clusters[sort_idx]\n",
    "\n",
    "    np.save(os.path.join(AIND_folder, 'spike_times.npy'),    spike_times)\n",
    "    np.save(os.path.join(AIND_folder, 'spike_clusters.npy'), spike_clusters)\n",
    "    unit_labels_df.to_csv(os.path.join(AIND_folder, 'cluster_group.tsv'),\n",
    "                         sep='\\t', index=False)\n",
    "\n",
    "    # -------- Stitch template metrics across segments (GLOBAL-keyed) --------\n",
    "    if len(tm_all_segments) > 0:\n",
    "        tm_all_df = pd.concat(tm_all_segments, ignore_index=True).drop_duplicates(subset=['global_unit_ids'])\n",
    "        tm_all_df.to_csv(os.path.join(AIND_folder, \"template_metrics.tsv\"), sep=\"\\t\", index=False)\n",
    "    else:\n",
    "        tm_all_df = None\n",
    "\n",
    "    # -------- Optional: stitch unit locations similarly --------\n",
    "    if len(uloc_all_segments) > 0:\n",
    "        uloc_all_df = pd.concat(uloc_all_segments, ignore_index=True).drop_duplicates(subset=['global_unit_ids'])\n",
    "    else:\n",
    "        uloc_all_df = None\n",
    "\n",
    "    # -------- Build best_channel_templates.npy and its metadata --------\n",
    "    if len(best_templates_all) > 0:\n",
    "        # normalize lengths to the most common (pad with zeros or truncate)\n",
    "        lengths = pd.Series([len(x) for x in best_templates_all])\n",
    "        mode_len = int(lengths.mode().iloc[0])\n",
    "        def _fixlen(x, L):\n",
    "            if len(x) == L: return x\n",
    "            if len(x) > L:  return x[:L]\n",
    "            out = np.zeros(L, dtype=np.float32); out[:len(x)] = x; return out\n",
    "        best_arr = np.vstack([_fixlen(x, mode_len)[None, :] for x in best_templates_all]).astype(np.float32)\n",
    "        np.save(os.path.join(AIND_folder, \"best_channel_templates.npy\"), best_arr)\n",
    "\n",
    "        best_meta_df = pd.DataFrame(best_templates_meta)\n",
    "        # keep the latest row per global_unit_ids (in case of duplicates across segments)\n",
    "        best_meta_df = best_meta_df.sort_values('segment_name').drop_duplicates(subset=['global_unit_ids'], keep='last')\n",
    "        # NOTE: you asked not to keep an extra TSV; we only use this df to merge onto cluster_info\n",
    "    else:\n",
    "        best_meta_df = None\n",
    "\n",
    "    # -------- Final cluster_info.tsv = QC + template metrics + best-channel info (deduped merges) --------\n",
    "    merged = qm_combined_df.copy()  # contains 'unit_ids' (LOCAL) + 'global_unit_ids'\n",
    "\n",
    "    # 1) Template metrics (GLOBAL-keyed) â€” avoid bringing unit_id/unit_ids again\n",
    "    if tm_all_df is not None and not tm_all_df.empty:\n",
    "        tm_cols_add = [c for c in tm_all_df.columns if c not in {'unit_id', 'unit_ids'}]\n",
    "        merged = merged.merge(tm_all_df[tm_cols_add], on='global_unit_ids', how='left')\n",
    "\n",
    "    # 2) Unit locations â€” only add if x_um/y_um not already present from the per-segment QC step\n",
    "    if uloc_all_df is not None and not uloc_all_df.empty and not ({'x_um','y_um'} <= set(merged.columns)):\n",
    "        if {'x_um','y_um'}.issubset(uloc_all_df.columns):\n",
    "            loc_use = uloc_all_df[['global_unit_ids','x_um','y_um']]\n",
    "        elif {'x','y'}.issubset(uloc_all_df.columns):\n",
    "            loc_use = uloc_all_df[['global_unit_ids','x','y']].rename(columns={'x':'x_um','y':'y_um'})\n",
    "        else:\n",
    "            loc_use = None\n",
    "        if loc_use is not None:\n",
    "            merged = merged.merge(loc_use, on='global_unit_ids', how='left')\n",
    "\n",
    "    # 3) Best-channel metadata â€” drop conflicting names first to prevent _x/_y\n",
    "    if best_meta_df is not None and not best_meta_df.empty:\n",
    "        add_cols = ['global_unit_ids', 'peak_channel', 'segment_name', 'n_samples', 'trough_to_peak_ms']\n",
    "        add_cols = [c for c in add_cols if c in best_meta_df.columns]\n",
    "        # drop any existing versions (except the key) so the merge won't create _x/_y\n",
    "        merged = merged.drop(columns=[c for c in add_cols if c != 'global_unit_ids' and c in merged.columns],\n",
    "                             errors='ignore')\n",
    "        merged = merged.merge(best_meta_df[add_cols], on='global_unit_ids', how='left')\n",
    "\n",
    "    # 4) Sanity: warn if any suffixed dup columns slipped in\n",
    "    suffixed = [c for c in merged.columns if c.endswith('_x') or c.endswith('_y')]\n",
    "    if suffixed:\n",
    "        print(f\"âš ï¸ Duplicated columns with suffixes detected (first 10): {suffixed[:10]}\")\n",
    "\n",
    "    # 5) Ensure global_unit_ids is the LAST column\n",
    "    cols = merged.columns.tolist()\n",
    "    if 'global_unit_ids' in cols:\n",
    "        cols.remove('global_unit_ids')\n",
    "        cols.append('global_unit_ids')\n",
    "        merged = merged[cols]\n",
    "\n",
    "    merged.to_csv(os.path.join(AIND_folder, 'cluster_info.tsv'), sep='\\t', index=False)\n",
    "    print(\"ğŸ¯ Wrote: spike_times.npy, spike_clusters.npy, cluster_group.tsv, cluster_info.tsv (+ template_metrics.tsv, best_channel_templates.npy)\")\n",
    "\n",
    "    # -------- analysis_meta.json (provenance; complements ap.meta) --------\n",
    "    ap_meta_candidates = glob.glob(os.path.join(raw_rec, \"*ap.meta\"))\n",
    "    ap_meta_path = ap_meta_candidates[0] if ap_meta_candidates else None\n",
    "\n",
    "    analysis_meta = {\n",
    "        \"notebook\": \"extract_aind_output_shijia_ks4.ipynb\",\n",
    "        \"export_time_iso\": time.strftime(\"%Y-%m-%dT%H:%M:%S\", time.localtime()),\n",
    "        \"paths\": {\n",
    "            \"raw_folder\": raw_rec,\n",
    "            \"export_folder\": AIND_folder,\n",
    "            \"ap_meta_path\": ap_meta_path,\n",
    "            \"preprocessed\": preProcessed,\n",
    "            \"postprocessed\": postProcessed,\n",
    "            \"spikesorted\": spikes,\n",
    "            \"curated\": curated,\n",
    "        },\n",
    "        \"recording_meta\": _parse_ap_meta(ap_meta_path),\n",
    "        \"waveform_params\": {\n",
    "            \"computed_extensions_used\": [\"templates\", \"template_metrics\"],   # used per segment\n",
    "            \"fallback_compute\": False\n",
    "        },\n",
    "        \"aggregation\": {\n",
    "            \"global_unit_count\": int(merged[\"global_unit_ids\"].nunique()) if not merged.empty else 0,\n",
    "            \"segments_count\": int(len(experiment_files)),\n",
    "        },\n",
    "        \"environment\": {\n",
    "            \"python_version\": platform.python_version(),\n",
    "            \"numpy_version\": np.__version__,\n",
    "            \"spikeinterface_version\": getattr(si, \"__version__\", \"unknown\"),\n",
    "        }\n",
    "    }\n",
    "\n",
    "    with open(os.path.join(AIND_folder, \"analysis_meta.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(analysis_meta, f, indent=2)\n",
    "    print(f\"ğŸ§¾ Saved analysis_meta.json in {AIND_folder}\")\n",
    "\n",
    "    # -------- Diagnostics: confirm template_metrics merged into cluster_info --------\n",
    "    check_template_metrics_merge(AIND_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87817d0-94c6-4056-8640-2af54696293e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$input_base_dir\"\n",
    "parent=$(dirname \"$1\")\n",
    "cd \"$1\"\n",
    "for d in */; do\n",
    "  # strip trailing slash for cleaner output\n",
    "  dname=${d%/}\n",
    "  mv \"$dname\" \"$parent/$dname\"\n",
    "  echo \"moved $dname to $parent/$dname\"\n",
    "done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de29699c-49f3-4055-a8f9-26c181c74808",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Transfer output file back: \n",
    "# change simlink:\n",
    "# cp -rL /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch /n/netscratch/bsabatini_lab/Lab/shliu/aind_output_scratch_globus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2a8185-625f-4f67-81dd-84f0684348d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce874230-16f5-4920-bf90-4feadf9d2a35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef85824-bff2-44d8-a937-3771863ea8ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (spikeInterface 0.102.1)",
   "language": "python",
   "name": "spikeinterface"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
