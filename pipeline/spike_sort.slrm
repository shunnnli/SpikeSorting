#!/bin/bash
#SBATCH --nodes=1
#SBATCH --mem=32G
#SBATCH --partition=kempner
#SBATCH --account=kempner_bsabatini_lab 
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=2
#SBATCH --gres=gpu:1
#SBATCH --time=24:00:00
#SBATCH --output=ephys-%N.%x.%j.out
#SBATCH --error=ephys-%N.%x.%j.err
#SBATCH --mail-user shunli@g.harvard.edu

# ================================
# Run on Kempner cluster with this command (tmux recommended for running in background):
# tmux new -s spikesort
# pipeline/shun_spikesort_pipeline.sh pipeline/spike_sort.slrm
# # detach with:  Ctrl-b d (both mac and windows)
# # reattach later: tmux attach -t spikesort
# ================================

DATA_PATH="/n/netscratch/bsabatini_lab/Lab/shunnnli/spikesorting/aind_input/todo/"
WORK_DIR="/n/netscratch/bsabatini_lab/Lab/shunnnli/spikesorting/work_dir/"
PIPELINE_PATH="pipeline/"
RESULTS_PATH="/n/netscratch/bsabatini_lab/Lab/shunnnli/spikesorting/aind_output_scratch/"
BACKUP_PATH="/n/netscratch/bsabatini_lab/Lab/shunnnli/spikesorting/aind_input/"
CONTAINER_DIR="/n/holylfs06/LABS/kempner_shared/Everyone/workflow/ephys-spike-sorting-2024/containers"
INPUT_DATA_TYPE="spikeglx"

# ================================
# Use custom preprocessing script (with manual bad channels support)

# To look up outputs of preprocessing script, run the following command:
# 1. Find the work_dir for preprocessng script: cat nextflow.log | grep "preprocessing"
# 2. Go to work_dir and find the output of preprocessing script: grep "\[custom\]" .command.log
# ================================
# Set to "true" to use run_capsule_custom.py (supports --bad-channel-ids)
# Set to "false" to use the default aind-ephys-preprocessing script
USE_CUSTOM_PREPROCESSING="true"

module load Mambaforge/23.11.0-fasrc01
mamba activate /n/holylfs06/LABS/kempner_shared/Everyone/workflow/ephys-spike-sorting-2024/versions/latest/software/nextflow_v22.10.6

# ================================
# Exclude last N seconds from the recording (default: 0 seconds)
# ================================
# Session-specific values are configured in pipeline/exclude_seconds.conf
# The pipeline script (shun_spikesort_pipeline.sh) will automatically set
# the correct value for each session when generating job scripts.
EXCLUDE_LAST_SEC="0"
export DATA_PATH
export EXCLUDE_LAST_SEC

# Compute duration from SpikeGLX meta files (fileTimeSecs=...)
# Search recursively for *.ap.meta files (they may be in imec0/, imec1/ subdirectories)
T_STOP=$(python3 - <<'PY'
import glob, os

data_path = os.environ["DATA_PATH"]
exclude = float(os.environ.get("EXCLUDE_LAST_SEC", "0"))

# Search recursively for *.ap.meta files
meta_files = sorted(glob.glob(os.path.join(data_path, "**", "*.ap.meta"), recursive=True))
if not meta_files:
    raise SystemExit(f"No *.ap.meta found in {data_path} or subdirectories (expected SpikeGLX AP meta files).")

durs = []
for mf in meta_files:
    try:
        with open(mf, "r") as f:
            for line in f:
                if line.startswith("fileTimeSecs"):
                    durs.append(float(line.split("=", 1)[1].strip()))
                    break
    except Exception as e:
        continue  # skip files that can't be read

if not durs:
    raise SystemExit(f"Could not parse fileTimeSecs from any *.ap.meta in {data_path}")

dur = min(durs)  # safest if multiple probes/streams differ slightly
t_stop = max(0.0, dur - exclude)
print(t_stop)
PY
)

# Verify T_STOP was computed successfully
if [ -z "$T_STOP" ] || ! [[ "$T_STOP" =~ ^[0-9]+\.?[0-9]*$ ]]; then
    echo "âŒ Failed to compute T_STOP from *.ap.meta files. Cannot proceed."
    exit 1
fi

echo "Cropping: keep [0, ${T_STOP}] seconds (excluding last ${EXCLUDE_LAST_SEC}s)"


# ================================
# Exclude first N seconds from the recording (default: 0 seconds)
# ================================
EXCLUDE_FIRST_SEC="0"
export EXCLUDE_FIRST_SEC

# ================================
# Bad channel IDs (session-specific, injected by wrapper script)
# Format: comma-separated channel indices (e.g., "20,206,3,313")
# ================================
BAD_CHANNELS=""
export BAD_CHANNELS

# ================================
# Build preprocessing args
# ================================
# Build preprocessing args
# PREPROCESSING_ARGS="--denoising cmr --t-start ${EXCLUDE_FIRST_SEC} --t-stop ${T_STOP}"
# PREPROCESSING_ARGS="--denoising cmr --t-start ${EXCLUDE_FIRST_SEC} --t-stop ${T_STOP} --no-remove-bad-channels"
# PREPROCESSING_ARGS="--denoising destripe --t-start ${EXCLUDE_FIRST_SEC} --t-stop ${T_STOP}"
PREPROCESSING_ARGS="--denoising destripe --t-start ${EXCLUDE_FIRST_SEC} --t-stop ${T_STOP} --no-remove-bad-channels"

# Append bad channels if provided (only when using custom preprocessing)
if [ -n "${BAD_CHANNELS}" ]; then
    PREPROCESSING_ARGS="${PREPROCESSING_ARGS} --bad-channel-ids ${BAD_CHANNELS}"
fi

/n/holylfs06/LABS/kempner_shared/Everyone/workflow/ephys-spike-sorting-2024/cred/kachery_shared_setup.sh

CONTAINER_DIR=$CONTAINER_DIR DATA_PATH=$DATA_PATH RESULTS_PATH=$RESULTS_PATH nextflow  -C $PIPELINE_PATH/nextflow_slurm.config \
    -log $RESULTS_PATH/nextflow.log \
    run $PIPELINE_PATH/main_slurm.nf\
    -work-dir $WORK_DIR \
    --n_jobs 4 \
    --sorter kilosort4 \
    --runmode full \
    --job_dispatch_args "--input $INPUT_DATA_TYPE" \
    --preprocessing_args "$PREPROCESSING_ARGS" \
    --use_custom_preprocessing "$USE_CUSTOM_PREPROCESSING" \
  
    
