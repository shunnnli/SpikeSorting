#!/bin/bash
#SBATCH --nodes=1
#SBATCH --mem=32G
#SBATCH --partition=kempner
#SBATCH --account=kempner_bsabatini_lab 
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=2
#SBATCH --gres=gpu:1
#SBATCH --time=24:00:00
#SBATCH --output=ephys-%N.%x.%j.out
#SBATCH --error=ephys-%N.%x.%j.err
#SBATCH --mail-user shunli@g.harvard.edu

DATA_PATH="/n/netscratch/bsabatini_lab/Lab/shunnnli/spikesorting/aind_input/todo/"
WORK_DIR="/n/netscratch/bsabatini_lab/Lab/shunnnli/spikesorting/work_dir/"
PIPELINE_PATH="pipeline/"
RESULTS_PATH="/n/netscratch/bsabatini_lab/Lab/shunnnli/spikesorting/aind_output_scratch/"
BACKUP_PATH="/n/netscratch/bsabatini_lab/Lab/shunnnli/spikesorting/aind_input/"


CONTAINER_DIR="/n/holylfs06/LABS/kempner_shared/Everyone/workflow/ephys-spike-sorting-2024/containers"

INPUT_DATA_TYPE="spikeglx"

module load Mambaforge/23.11.0-fasrc01
mamba activate /n/holylfs06/LABS/kempner_shared/Everyone/workflow/ephys-spike-sorting-2024/versions/latest/software/nextflow_v22.10.6

# --- Exclude last N seconds from the recording (default: 200 seconds) ---
# Random4: 600s done
# Reward1: 350s done    with 10ms: 281s
# Reward2: 400s done    with 10ms: 315s
# Reward3: 165s         with 10ms: 112s
# Reward4: 250s         with 10ms: 168s
# Punish1: 250s         with 10ms: 170s
# Punish2: 250s         with 10ms: 198s
# Punish3: 250s         with 10ms: 176s
# Punish4: 250s         with 10ms: 204s
# Reward5: 305s         with 10ms: 255s
# Punish5: 300s         with 10ms: 229s
# Reward6: 380s         with 10ms: 314s
# Punish6: 250s         with 10ms: 187s
EXCLUDE_LAST_SEC="${EXCLUDE_LAST_SEC:-281}"

# Export variables so they're available to Python subprocess
export DATA_PATH
export EXCLUDE_LAST_SEC

# Compute duration from SpikeGLX meta files (fileTimeSecs=...)
# Search recursively for *.ap.meta files (they may be in imec0/, imec1/ subdirectories)
T_STOP=$(python3 - <<'PY'
import glob, os

data_path = os.environ["DATA_PATH"]
exclude = float(os.environ.get("EXCLUDE_LAST_SEC", "200"))

# Search recursively for *.ap.meta files
meta_files = sorted(glob.glob(os.path.join(data_path, "**", "*.ap.meta"), recursive=True))
if not meta_files:
    raise SystemExit(f"No *.ap.meta found in {data_path} or subdirectories (expected SpikeGLX AP meta files).")

durs = []
for mf in meta_files:
    try:
        with open(mf, "r") as f:
            for line in f:
                if line.startswith("fileTimeSecs"):
                    durs.append(float(line.split("=", 1)[1].strip()))
                    break
    except Exception as e:
        continue  # skip files that can't be read

if not durs:
    raise SystemExit(f"Could not parse fileTimeSecs from any *.ap.meta in {data_path}")

dur = min(durs)  # safest if multiple probes/streams differ slightly
t_stop = max(0.0, dur - exclude)
print(t_stop)
PY
)

# Verify T_STOP was computed successfully
if [ -z "$T_STOP" ] || ! [[ "$T_STOP" =~ ^[0-9]+\.?[0-9]*$ ]]; then
    echo "âŒ Failed to compute T_STOP from *.ap.meta files. Cannot proceed."
    exit 1
fi

echo "Cropping: keep [0, ${T_STOP}] seconds (excluding last ${EXCLUDE_LAST_SEC}s)"

# Build preprocessing args (add your other preprocessing flags here too)
PREPROCESSING_ARGS="--denoising cmr --t-start 0 --t-stop ${T_STOP}"


/n/holylfs06/LABS/kempner_shared/Everyone/workflow/ephys-spike-sorting-2024/cred/kachery_shared_setup.sh

CONTAINER_DIR=$CONTAINER_DIR DATA_PATH=$DATA_PATH RESULTS_PATH=$RESULTS_PATH nextflow  -C $PIPELINE_PATH/nextflow_slurm.config \
    -log $RESULTS_PATH/nextflow.log \
    run $PIPELINE_PATH/main_slurm.nf\
    -work-dir $WORK_DIR \
    --n_jobs 4 \
    --sorter kilosort4 \
    --runmode full \
    --job_dispatch_args "--input $INPUT_DATA_TYPE" \
    --preprocessing_args "$PREPROCESSING_ARGS" \
  
    
